\chapter{Introduction}
L'optimisation différentiable est une branche assez spécifique des mathématiques qui est importante dans de nombreux domaines où l'on souhaite maximiser ou minimiser des fonctions, c'est-à-dire, résoudre un problème d'optimisation.\\

 Ces problèmes ont un champ d'application très large comme par exemple en algèbre linéaire où l'on souhaite résoudre des problèmes du type $Ax=b$. Mais également dans des problèmes plus concrets et physiques comme la conception optimale de systèmes, ou dans la recherche opérationnelle. Ainsi, il est essentiel de trouver des méthodes efficaces pour résoudre ces problèmes. Il existe une grande variété d'algorithmes permettant de minimiser (ou maximiser puisque c'est équivalent), une fonction différentiable donnée.\\

 Si les algorithmes les plus communs sont plus souvent des méthodes de descente de gradient, d'autres algorithmes peuvent être plus efficaces dans des conditions spécifiques, c'est le cas des algorithmes de Newton et de Quasi-Newton (par exemple les méthodes du types BFGS telles que L-BFGS, très utilisé dans l'industrie).\\


Nous nous intéresserons tout d'abord à la théorie justifiant ces méthodes avant de les appliquer à un domaine pour lequel l'intérêt ressurgit depuis quelques années : le Machine Learning.
