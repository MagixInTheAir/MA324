\chapter{Introduction}
L'optimisation différentiable est une branche assez spécifique des mathématiques qui est importante dans de nombreux domaines où l'on souhaite maximiser ou minimiser des fonctions, c'est-à-dire, résoudre un problème d'optimisation.\\

 Ces problèmes ont un champ d'application très large comme par exemple en algèbre linéaire où l'on souhaite résoudre des problèmes du type $Ax=b$ ou dans des problèmes plus concrets et physiques comme la conception optimale de systèmes ou dans la recherche opérationnelle. Ainsi, il est essentiel de trouver des méthodes efficaces afin de résoudre ces problèmes. Il existe une grande variété d'algorithmes permettant de minimiser (ou maximiser puisque cela revient au même), une fonction différentiable donnée.\\

 Si les algorithmes les plus communes sont plus souvent des méthodes de descentes de gradients, d'autres algorithmes peuvent être plus efficaces dans des conditions spécifiques, c'est le cas des algorithmes de Newton et de Quasi-Newton (L-BFGS par exemple).\\



Nous nous intéresserons tout d'abord à la théorie justifiant ces méthodes avant de les appliquer à un domaine pour lequel l'intérêt resurgit depuis quelques années : le Machine Learning.
