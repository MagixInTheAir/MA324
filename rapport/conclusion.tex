\chapter{Conclusion}

Ce mémoire nous a permis de faire une présentation des différentes méthodes de descentes utilisées afin de résoudre différent problèmes d'optimisation et notamment de minimisation. En fonction du problème étudié et de sa dimension, nous avons différents outils à notre disposition afin de trouver le minimiseur d'une fonction. \\

Nous avons donc développé principalement l'utilisation de la méthode du gradient-conjugué ainsi que les principales caractéristiques de celle-ci. Pour ce faire, nous avons établit et défini un outil mathématique très important en minimisation : les espaces de krylov qui nous permettent de définir d'importantes propriétés de convergence pour ces algorithmes.\\

 Nous avons aussi chercher quelques pistes d'amélioration avec l'utilisation des algorithmes de Newton. Néanmoins, étant assez lourd à effectuer, nous avons exposé le principe des algorithmes de Quasi-Newton qui permettent d'approximer la hessienne. Cela nous produit une catégorie d'algorithmes possédant les propriétés de ceux de Newton tout en étant moins lourd.\\

Ensuite, nous avons choisi d'appliquer ce domaine à un domaine très vaste et en expansion de nos jours. En effet, dès le début de notre étude des intelligences artificielles, nous nous sommes aperçu que ce n'était que des mathématiques d'optimisation que nous utilisons dans différents buts. Nous avons donc appliqué cela à plusieurs problèmes. 