\chapter{Conclusion}

Ce mémoire nous a permis de faire une présentation des différentes méthodes de descente utilisées afin de résoudre différents problèmes d'optimisation et notamment de minimisation. En fonction du problème étudié et de sa dimension, nous avons un certain nombre d'outils à notre disposition afin de trouver le minimiseur d'une fonction. \\

Nous avons donc développé principalement l'utilisation de la méthode du gradient-conjugué ainsi que les principales caractéristiques de celle-ci. Pour ce faire, nous avons établi et défini un outil mathématique très important en minimisation : les espaces de Krylov qui nous permettent de définir d'importantes propriétés de convergence pour ces algorithmes.\\

Nous avons aussi cherché quelques pistes d'amélioration avec l'utilisation des algorithmes de Newton. Néanmoins, le calcul étant assez lourd à effectuer, nous avons exposé le principe des algorithmes de Quasi-Newton qui permettent d'approximer la matrice hessienne. Cela nous produit une famille d'algorithmes possédant les propriétés de ceux de Newton tout en étant moins lourds.\\

Enfin, nous avons choisi d'appliquer ces études à un domaine très vaste et en expansion depuis quelques années. En effet, dès le début de notre étude des intelligences artificielles, nous nous sommes aperçus que ce n'était que des mathématiques d'optimisation que nous utilisons dans différents buts. Nous avons donc appliqué cela à un problème concret qu'est la reconnaissance d'objets. 